# SPDX-License-Identifier: Apache-2.0
# Copyright(c) 2021-2024 Wind River Systems, Inc.
- name: Deployment Manager Playbook
  hosts: all
  gather_facts: false
  become: false

  vars:
    helmv2_installed: false
    helmv3_installed: false

  tasks:
    - set_fact:
        manager_chart: "{{ deployment_manager_chart | default('wind-river-cloud-platform-deployment-manager.tgz') }}"

    - set_fact:
        dm_monitor_playbook: "{{ dm_monitor_playbook | default('/usr/local/share/applications/playbooks/dm-monitor-book.yaml') }}"

    - set_fact:
        user_uploaded_artifacts: "{{ user_uploaded_artifacts | default(true) }}"

    - set_fact:
        ansible_port: "{{ ansible_port | default(22) }}"
        wait_for_timeout: " {{ wait_for_timeout | default(400) }}"
        boot_wait_time: " {{ boot_wait_time | default(180) }}"

    - set_fact:
        helm_chart_overrides: "{{ deployment_manager_overrides }}"
      when: deployment_manager_overrides is defined

    - set_fact:
        deploy_config: "{{ deployment_config }}"
        wait_for_dm_unlock: " {{ wait_for_dm_unlock| default(5) }}"
      when: deployment_config is defined

    - block:
      # Information to be used at final DM checks.
        - name: Get host name
          shell: |
            source /etc/platform/openrc; hostname
          register: get_host_name

        # In order to determine if the host is a subcloud's node
        - name: Get distributed_cloud_role
          shell: >-
            source /etc/platform/openrc; system show |
            awk '$2 ~ /^distributed_cloud_role/ {print $4}'
          register: get_distributed_cloud_role

        - name: Show dc_role and hostname
          debug:
            msg:
            - "hostname: {{get_host_name.stdout}}"
            - "dc_role: {{ get_distributed_cloud_role.stdout }}"

    - block:
      # Copy required files up to the target host if this playbook is being
      # executed remotely.

      - block:
        # Download a copy of the deployment manager helm chart if the location
        # supplied by the end user references a git repo.

        - name: Create A Temporary Download Directory
          tempfile:
            state: directory
          register: temp
          delegate_to: localhost

        - name: Download Deployment Manager Helm Chart From Repo
          shell: "git archive --remote={{ manager_chart }} | tar -x -C {{ temp.path }}"
          delegate_to: localhost

        - name: Reference Downloaded Helm Chart
          set_fact:
            manager_chart: "{{ lookup('fileglob', '{{ temp.path }}/docs/charts/wind-river-cloud-platform-deployment-manager-*.tgz', wantlist=true) | first }}"

        when: manager_chart | regex_search("^https|^git")

      - name: Upload Deployment Manager Helm Chart
        copy:
          src: "{{ manager_chart }}"
          dest: /home/{{ ansible_ssh_user }}/
          owner: "{{ ansible_ssh_user }}"
          group: root
          mode: 0644

      - set_fact:
          manager_chart: "/home/{{ ansible_ssh_user }}/{{ manager_chart | basename }}"

      - name: Upload Deployment Manager Helm Chart Overrides
        copy:
          src: "{{ helm_chart_overrides }}"
          dest: /home/{{ ansible_ssh_user }}/
          group: root
          mode: 0644
        when: helm_chart_overrides is defined

      - set_fact:
          helm_chart_overrides: "/home/{{ ansible_ssh_user }}/{{ helm_chart_overrides | basename }}"
        when: helm_chart_overrides is defined

      - block:

        ## user not uploaded the artifacts
        ## check in subcloud /usr/local/share/applications/

        - set_fact:
            helm_chart_overrides: "/usr/local/share/applications/overrides/wind-river-cloud-platform-deployment-manager-overrides-subcloud.yaml"
            manager_chart: "/usr/local/share/applications/helm/wind-river-cloud-platform-deployment-manager-*.tgz"

        - name: Check if helm-chart file exists in subcloud
          shell: ls {{ manager_chart }}
          ignore_errors: yes
          register: helm_chart_file

        - name: Check if helm-overrides file exists in subcloud
          stat:
            path: "{{ helm_chart_overrides }}"
          register: helm_chart_overrides_file

        - name: Fail helm-chart and overrides not exists in subcloud
          fail:
            msg: "Helm-chart and overrides not exists in subcloud"
          when: (helm_chart_file.stdout | length == 0 or helm_chart_overrides_file.stat.exists == False)

        when: ("subcloud" in get_distributed_cloud_role.stdout and user_uploaded_artifacts is false)

      - name: Clean download directory
        file:
          path: "{{ temp.path }}"
          state: absent
        delegate_to: localhost
        when: temp.path is defined

      when: inventory_hostname != 'localhost'

    - name: Retrieve software version number
      shell: source /etc/build.info; echo $SW_VERSION
      register: sw_version_result

    - name: Fail if software version is not defined
      fail:
        msg: "SW_VERSION is missing in /etc/build.info"
      when: sw_version_result.stdout_lines|length == 0

    - name: Set software version and platform path
      set_fact:
        software_version: "{{ sw_version_result.stdout_lines[0] }}"
        platform_path: /opt/platform

    - name: Set config path facts
      set_fact:
        config_permdir: "{{ platform_path + '/config/' + software_version }}"

    # Check if DM already exists in either helmv2 or helmv3
    - name: Get list of releases in helmv2
      command: >-
        /usr/local/sbin/helmv2-cli -- helm list --output json
      environment:
        KUBECONFIG: "/etc/kubernetes/admin.conf"
      ignore_errors: yes
      register: helmv2_releases

    # Ansible returns error code 2 to helmv2_releases if helmv2-cli does not exist
    - name: Check if helmv2-cli is installed
      set_fact:
        helmv2_installed: "{{ false if helmv2_releases.rc == 2 else true }}"

    - name: Query for DM release in helmv2
      shell: |
        grep -iq '"name":"deployment-manager"' <<< {{ helmv2_releases.stdout_lines }}
      ignore_errors: yes
      register: helmv2_dm_exists
      when: helmv2_installed

    - name: Get list of releases in helmv3
      command: >-
        /usr/sbin/helm list --output json
      environment:
        KUBECONFIG: "/etc/kubernetes/admin.conf"
      ignore_errors: yes
      register: helmv3_releases

    # Ansible returns error code 2 to helmv3_releases if helmv3 does not exist
    - name: Check if helmv3 is installed
      set_fact:
        helmv3_installed: "{{ false if helmv3_releases.rc == 2 else true }}"

    - name: Query for DM release in helmv3
      shell: |
        grep -iq '"name":"deployment-manager"' <<< {{ helmv3_releases.stdout_lines }}
      ignore_errors: yes
      register: helmv3_dm_exists
      when: helmv3_installed

    - name: Check nfv-vim.dor_complete flag
      stat:
        path: /var/run/.nfv-vim.dor_complete
      register: dor_complete_file

    - name: Set reconfiguration related facts
      set_fact:
        reconfig_flag: "{{ true if (helmv2_installed and helmv2_dm_exists.rc == 0)
                        or (helmv3_installed and helmv3_dm_exists.rc == 0)
                        else false }}"
        initial_deploy: "{{ false if dor_complete_file.stat.exists else true }}"

    - name: Mark the bootstrap as finalized
      file:
        path: "{{ config_permdir }}/.bootstrap_finalized"
        state: touch
      become: yes
      when: not reconfig_flag

    - block:
      - name: Search for the pod of the Deployment Manager
        shell: |
          kubectl -n platform-deployment-manager get pods | grep platform-deployment-manager- | awk 'NR == 1 { print $1 }'
        environment:
          KUBECONFIG: "/etc/kubernetes/admin.conf"
        register: deployment_manager_pod_name

      - debug:
          msg: "{{ deployment_manager_pod_name.stdout }}"
      when: reconfig_flag

    # Follow a different reinstallation procedure if DM is installed in helmv2
    - block:
      - name: Get armada pod name
        command: >-
          kubectl get pods -n armada -o custom-columns=NAME:metadata.name --no-headers
        environment:
          KUBECONFIG: "/etc/kubernetes/admin.conf"
        register: armada_pod

      - name: Show armada pod
        debug:
          msg:
          - "armada pod ID: {{ armada_pod.stdout }}"

      - name: Copy files into tiller container if using helmv2
        command: "{{ item }}"
        with_items:
          - kubectl -n armada cp
            {{ manager_chart }}
            {{ armada_pod.stdout }}:/tmp/.
            -c tiller
          - kubectl -n armada cp
            {% if helm_chart_overrides is defined %}{{ helm_chart_overrides }}{% endif %}
            {{ armada_pod.stdout }}:/tmp/.
            -c tiller
        environment:
          KUBECONFIG: "/etc/kubernetes/admin.conf"

      - name: Reinstall Deployment Manager (helmv2)
        command: >-
          /usr/local/sbin/helmv2-cli -- helm upgrade
          --install deployment-manager
          --values /tmp/{{ helm_chart_overrides | basename }}
          /tmp/{{ manager_chart | basename }}
        environment:
          KUBECONFIG: "/etc/kubernetes/admin.conf"

      when: helmv2_installed and helmv2_dm_exists.rc == 0

    # Remove v1 webhook and webhook service
    - block:
      - name: Search webhook configurations for v1
        shell: |
          kubectl get ValidatingWebhookConfigurations validating-webhook-configuration --no-headers | awk 'NR == 1 { print $1 }'
        environment:
          KUBECONFIG: "/etc/kubernetes/admin.conf"
        ignore_errors: yes
        register: validation_webhook

      - name: Query for validating webhook configuration
        shell: |
          grep -iq 'validating-webhook-configuration' <<< {{ validation_webhook.stdout_lines }}
        ignore_errors: yes
        register: webhook_config_exists

      - name: Delete validating webhook configuration
        shell: |
          kubectl delete ValidatingWebhookConfigurations validating-webhook-configuration
        environment:
          KUBECONFIG: "/etc/kubernetes/admin.conf"
        when: webhook_config_exists.rc == 0

      - name: Search webhook service for v1
        shell: |
          kubectl get svc webhook-server-service -n platform-deployment-manager --no-headers | awk 'NR == 1 { print $1 }'
        environment:
          KUBECONFIG: "/etc/kubernetes/admin.conf"
        register: webhook_service

      - name: Query for webhook-server-service
        shell: |
          grep -iq 'webhook-server-service' <<< {{ webhook_service.stdout_lines }}
        ignore_errors: yes
        register: webhook_service_exists

      - name: Delete webhook-server-service
        shell: |
          kubectl delete svc webhook-server-service -n platform-deployment-manager
        environment:
          KUBECONFIG: "/etc/kubernetes/admin.conf"
        when: webhook_service_exists.rc == 0

      when: helmv3_installed and helmv3_dm_exists.rc == 0

    - name: Install Deployment Manager
      shell: KUBECONFIG=/etc/kubernetes/admin.conf /usr/sbin/helm upgrade --install deployment-manager {% if helm_chart_overrides is defined %}--values {{ helm_chart_overrides }}{% endif %} {{ manager_chart }}
      when: helmv3_installed

    # Install or Upgrade DM-Monitor
    - name: "Install or Upgrade DM-monitor"
      shell: "ansible-playbook {{ dm_monitor_playbook }}"
      become: yes
      ignore_errors: yes
      when: dm_monitor_playbook

    # Delete old Deployment Manager pod if it was reinstalled,
    # the pod to delete may already be terminated by helm
    - name: Restart Deployment Manager if reinstalled
      command: >-
        kubectl -n platform-deployment-manager delete pods {{ deployment_manager_pod_name.stdout }}
      environment:
        KUBECONFIG: "/etc/kubernetes/admin.conf"
      when: deployment_manager_pod_name.stdout is defined and deployment_manager_pod_name.stdout
      failed_when: false

    - name: Wait for Deployment Manager to be ready
      shell: KUBECONFIG=/etc/kubernetes/admin.conf /bin/kubectl wait --namespace=platform-deployment-manager --for=condition=Ready pods --selector control-plane=controller-manager --timeout=60s
      register: wait_for_deployment_manager
      ignore_errors: true

    - block:
      - name: Search for the Deployment Manager pod name in case of failure
        shell: |
          kubectl -n platform-deployment-manager get pods | awk 'NR == 2 { print $1 }'
        environment:
          KUBECONFIG: "/etc/kubernetes/admin.conf"
        register: deployment_manager_pod_name_after_fail

      - name: Describe DM pod in case of download failed
        shell: |
          kubectl -n platform-deployment-manager describe pod "{{ deployment_manager_pod_name_after_fail.stdout }}" |
          awk '/Failed/ || /error/  {print $0}'
        environment:
          KUBECONFIG: "/etc/kubernetes/admin.conf"
        register: describe_dm_failed_pod

      - name: Fail if Deployment Manager pod is not ready
        fail:
          msg:
            - "Deployment manager pod are not ready"
            - "Check helm overrides files and ensure you can pull the image from the specified URL."
            - "Err_code= dm_pod_failed"
            - "Pod information: {{describe_dm_failed_pod.stdout}}"
      when: wait_for_deployment_manager.stderr != ""

    - block:
        - name: Upload Deployment Configuration File
          copy:
            src: "{{ deploy_config }}"
            dest: /home/{{ ansible_ssh_user }}/deployment-config.yaml
            owner: "{{ ansible_ssh_user }}"
            group: root
            mode: 0755
          when: inventory_hostname != 'localhost'

        - set_fact:
            deploy_config: "/home/{{ ansible_ssh_user }}/deployment-config.yaml"
          when: inventory_hostname != 'localhost'

        - name: Configure required region value for subcloud
          block:
          - name: Get subcloud region value from environment
            shell: source /etc/platform/openrc; echo $OS_REGION_NAME
            register: os_region_name_from_env

          - name: Show subcloud region
            debug:
              msg:
              - "Subcloud region value : {{os_region_name_from_env.stdout}}"

          - name: Configure new subcloud region value
            command: |
              sed -i 's/^\(\s*OS_REGION_NAME:\).*/\1 {{ os_region_name_from_env.stdout }}/' {{ deploy_config }}
          when: ("subcloud" in get_distributed_cloud_role.stdout)

        - wait_for:
            # Pause for an arbitrary amount of time to allow the deployment
            # manager to come up and download its certificates.  It needs to
            # restart during this process so the webhooks may not be ready when
            # we apply the config in the next steps.
            timeout: 10
            msg: Waiting for the Deployment Manager validation webhooks to start

        # Search for both cases:
        #   deploymentScope: "principal"
        # and
        #   deploymentScope:
        #    "principal"
        - name: Search for 'principal' scope in deploy_config
          command: grep -Pzo '\n\s*deploymentScope:\s*\n*(?:\s*#.*\n)*\s*(?:"principal"|"Principal"|"PRINCIPAL")\s*' {{ deploy_config }}
          changed_when: false
          failed_when: false
          register: scope_principal

        # Check if there is any strategy-related alarm
        - name: Check if a VIM strategy is in progress
          shell: |
            source /etc/platform/openrc; fm alarm-list | grep 900.
          failed_when: false
          register: strategy_alarms

        - name: Fail if there is a strategy in progress during a Day-2 operation
          fail:
            msg:
              - "VIM Strategy in progress."
              - "Wait until all the strategies are cleared before applying the system configuration update."
          when: ( scope_principal.stdout_lines | length > 0 and strategy_alarms.stdout != "")

        - name: Apply Deployment Configuration File
          shell: KUBECONFIG=/etc/kubernetes/admin.conf /bin/kubectl apply -f {{ deploy_config }}
          register: apply_deploy_config
          retries: 5
          delay: 10
          until: apply_deploy_config.rc == 0
          ignore_errors: true

        - name: Fail if config file failed to apply
          fail:
            msg:
              - "Failed to apply DM config file"
              - "Check syntax into config file. See documentation examples. "
              - "Err_code= dm_apply_failed"
              - "Config file error: {{apply_deploy_config.stderr}}"
          when: apply_deploy_config.rc != 0

        # Waiting task after apply config to avoid failures getting info
        - wait_for:
            timeout: 5
            msg: Waiting after apply DM config

        # Check the current administrativestate
        - name: Get current administrativeState
          shell: |
            source /etc/platform/openrc;
            system host-show "{{ get_host_name.stdout}}" | awk '$2 ~ /^administrative/ {print $4}'
          register: current_administrativestate

      when: deploy_config is defined

    # Create default registry key in platform-deployment-manager for future image pulls
    - name: Get platform-deployment-manager namespace default registry key
      command: >-
        kubectl --kubeconfig=/etc/kubernetes/admin.conf get secret default-registry-key --namespace=platform-deployment-manager
      failed_when: false
      register: get_dm_default_registry_key

    - name: Copy default-registry-key to platform-deployment-manager namespace
      shell: >-
        kubectl get secret default-registry-key --namespace=kube-system -o yaml
        | sed 's/namespace: kube-system/namespace: platform-deployment-manager/'
        | kubectl apply --namespace=platform-deployment-manager -f -
      environment:
        KUBECONFIG: "/etc/kubernetes/admin.conf"
      when: get_dm_default_registry_key.stdout == ""

    # Pre-verification block: it will search for administrative state applied
    # in order to avoid waiting for unlock when config administrativeState=locked.
    # Search first in host resource config. If not present, search into hostprofile.
    - block:
        - name: Search administrativeState into host resource
          shell: |
            source /etc/platform/openrc;
            kubectl get host "{{ get_host_name.stdout}}" -n deployment -o=jsonpath='{.spec.administrativeState}'
          environment:
            KUBECONFIG: "/etc/kubernetes/admin.conf"
          register: get_host_adminstate
          ignore_errors: yes

        # If config not found in host -previous task- then search into hostprofile
        - name: Get hostprofile name for this host
          shell: |
            source /etc/platform/openrc;
            kubectl get host "{{get_host_name.stdout}}" -n deployment -o=jsonpath='{.spec.profile}'
          environment:
            KUBECONFIG: "/etc/kubernetes/admin.conf"
          register: get_hostprofile_name
          when: get_host_adminstate.stdout == ""
          ignore_errors: yes

        - name: Search administrativeState into hostprofile resource
          shell: |
            source /etc/platform/openrc;
            kubectl get hostprofile "{{get_hostprofile_name.stdout}}" -n deployment -o=jsonpath='{.spec.administrativeState}'
          environment:
            KUBECONFIG: "/etc/kubernetes/admin.conf"
          register: get_hostprofile_adminstate
          when: (get_hostprofile_name.stdout != "" and get_hostprofile_name.stderr == "")

        - set_fact:
            administrative_state: "{{get_host_adminstate.stdout + get_hostprofile_adminstate.stdout}}"
          when: (get_host_name.stderr == "" and get_hostprofile_name.stderr == "")

        - name: Show config administrativeState
          debug:
            msg:
            - "administrative state: {{administrative_state}}"

      when:
        - initial_deploy
        - deploy_config is defined
        - "'subcloud' in get_distributed_cloud_role.stdout"
        - "'unlocked' not in current_administrativestate.stdout"


      # Main verification block.
      # Behavior:
      # This block waits until the host unlock is triggered.
      # - If the unlock is not triggered after a certain time, it might indicate DM failures.
      #   In this case, it proceeds to the 'get failure information' block to collect details
      #   about the potential cause.
      # - If the unlock process is in progress, it proceeds to the 'unlock verifications' block.
      #   Here, it waits for the host's port to be closed and then reopened. Following that, it checks
      #   the status of resources before completing.
    - block:

        # - If unlock task is triggered, is highly probable that the config applied is correct.
        # - The task fails by achieving max retries without calling the unlock.
        - name: Wait until unlock task triggered
          shell: |
            source /etc/platform/openrc;
            system host-show "{{ get_host_name.stdout}}" | awk '$2 ~ /^task/ {print $4}'
          register: get_show_task_status
          until: ("Unlocking" in get_show_task_status.stdout)
          retries: 60
          delay: "{{wait_for_dm_unlock}}"
          ignore_errors: yes

        - name: Show waiting unlock
          debug:
            msg:
            - "waiting: {{get_show_task_status.stdout}}"
            - "waiting: {{get_show_task_status.stderr}}"

        # Get failure information Block:
        # if unlock was not triggered, get information and make the playbook fail below
        - block:

          # Get a list of unreconciled resources pre unlock
          # This task is getting the last column as RECONCILED value
          # TODO(ecandotti): Modify this block to not retrieve the last column, but instead recognize the column number for RECONCILED in each resource.
          # TODO(ecandotti): This will eliminate the need to always have the RECONCILED column as the last column when a new state is added.
          - name: Retrieve kubectl resources in unreconciled status
            shell: >-
              kubectl -n deployment get datanetworks,hostprofiles,hosts,platformnetworks,systems,ptpinstances,ptpinterfaces |
              awk '$NF ~ /^false/ {print}'
            environment:
              KUBECONFIG: "/etc/kubernetes/admin.conf"
            register: get_recon_status

          - name: Show unreconciled resources
            debug:
              msg:
              - "recon: {{get_recon_status.stdout}}"

          # Get pod name to retrieve the logs
          - name: Get DM pod name
            shell: >-
              kubectl -n platform-deployment-manager get pods |
              awk '$1 ~ /^platform-deployment-manager/ {print $1}'
            environment:
              KUBECONFIG: "/etc/kubernetes/admin.conf"
            register: get_dm_pod_name

          # Get errors from pod logs. Searching for error lines into logs which:
          # - Contain 'ERROR' key word.
          # - Are not the same "Verb + value". Usually we see same error
          # - Are not validation or waiting errors (which could be temporal).
          #   multiple times in logs.
          - name: Retrieve DM logs
            shell: >-
              kubectl -n platform-deployment-manager logs "{{ get_dm_pod_name.stdout }}" |
              awk '(/ERROR/ && !/validation/ && !/waiting/ && !/Reconciler error/&& !(seen[$10, $NF]++)) {print}'
            environment:
              KUBECONFIG: "/etc/kubernetes/admin.conf"
            register: get_logs_pre_unlock

          - name: Show logs
            debug:
              msg:
              - "Pod log: {{get_logs_pre_unlock.stdout}}"
              - "err: {{get_logs_pre_unlock.stderr}}"

          when: ("Unlocking" not in get_show_task_status.stdout)

        # Unlock verifications Block:
        # if unlock was triggered, wait until unlock complete
        # and check resources and logs.
        - block:

          # check close-open port before starting last checkings
          - block:
            - name: Waiting for port to be closed due to unlock task
              wait_for:
                port: "{{ ansible_port }}"
                host: "{{ ansible_host }}"
                timeout: "{{ wait_for_timeout }}"
                state: stopped

            - name: Waiting for port to become open
              local_action:
                module: wait_for
                  port={{ ansible_port }}
                  host={{ ansible_host }}
                  timeout={{ wait_for_timeout }}
                  state=started
              retries: 20
              delay: 20

            # Rescue block due to port timing issues:
            # It will check the administrative state and status of resources.
            # If the state is not "unlocked," log the information from the subcloud.
            # If the state is "unlocked," continue with the normal flow of the main block.
            rescue:
              - name: Waiting for port to become open
                local_action:
                  module: wait_for
                    port={{ ansible_port }}
                    host={{ ansible_host }}
                    delay={{ boot_wait_time }}
                    timeout={{ wait_for_timeout }}
                    state=started
                retries: 20
                delay: 20

              - name: Wait for {{ boot_wait_time }} seconds before checking status
                wait_for:
                  timeout: 300
                register: waiting_after_failures
                ignore_errors: true

              - name: Get post unlock state
                shell: >-
                  kubectl -n deployment get hosts "{{ get_host_name.stdout}}" |
                  awk 'NR == 1 { for (i=1; i<=NF; i++) { if ($i == "ADMINISTRATIVE") { col = i; break } } } NR > 1 { print $col }'
                environment:
                  KUBECONFIG: "/etc/kubernetes/admin.conf"
                register: get_final_admin_state
                until: ( administrative_state == get_final_admin_state.stdout)
                retries: 4
                delay: 15
                ignore_errors: yes

              - name: Retrieve kubectl resources reconciled status
                shell: >-
                  (kubectl -n deployment get datanetworks,platformnetworks,systems,ptpinstances,ptpinterfaces;
                  kubectl -n deployment get hosts "{{ get_host_name.stdout}}")
                environment:
                  KUBECONFIG: "/etc/kubernetes/admin.conf"
                register: resources_status
                ignore_errors: yes

              - name: Showing status after unlock
                debug:
                  msg:
                  - "WARNING. Final status after reboot: {{get_final_admin_state.stdout}}"
                  - "It's possible that the status after attempting to unlock is not as desired:"
                  - "{{resources_status.stdout}}"
                when: ( administrative_state != get_final_admin_state.stdout)

          # Waiting task after unlock to catch right status
          # This simple task usually doesn't fail. However, in some
          # cases, after unlocking, we may encounter two reboots. In such
          # situations, this task might fail due to a loss of connection.
          # If that happens, the following block will wait for the
          # subsequent reboot.
          - name: Wait for {{ boot_wait_time }} seconds to ensure not affecting host status
            wait_for:
              timeout: "{{ boot_wait_time }}"
            register: waiting_after_reboot
            ignore_errors: true

          # Retry block: sometimes system reboots twice
          # It will take some extra time.
          - block:
            - name: Waiting for port to become open on second reboot
              local_action:
                module: wait_for
                  port={{ ansible_port }}
                  host={{ ansible_host }}
                  delay={{ boot_wait_time }}
                  timeout={{ wait_for_timeout }}
                  state=started
              retries: 20
              delay: 20

            # Waiting task after unlock to catch right status
            - name: Wait for {{ boot_wait_time }} seconds to ensure not affecting host status
              wait_for:
                timeout: "{{ boot_wait_time }}"
              register: waiting_after_new_reboot
              ignore_errors: true

            - name: Fail in case of new loss of connection
              fail:
                msg:
                  - "Unexpected loss of connection. Check subcloud status manually."
                  - "Then, use subcloud reconfig command to complete deployment"
              register: new_reboot
              when: waiting_after_new_reboot.failed

            when: waiting_after_reboot.failed

          # After reboot, wait some time for resources to be reconciled.
          # If we have all of them reconciled, playbook won't fail.
          # This task is getting the last column as RECONCILED status value
          # TODO(ecandotti): Modify this block to not retrieve the last column, but instead recognize the column number for RECONCILED in each resource.
          # TODO(ecandotti): This will eliminate the need to always have the RECONCILED column as the last column when a new state is added.
          - name: Retrieve kubectl resources reconciled status
            shell: >-
              (kubectl -n deployment get datanetworks,platformnetworks,systems,ptpinstances,ptpinterfaces;
               kubectl -n deployment get hosts "{{ get_host_name.stdout}}") |
               awk '$NF ~ /^false/ {print}'
            environment:
              KUBECONFIG: "/etc/kubernetes/admin.conf"
            register: get_unrecon_status_post
            until: (get_unrecon_status_post.stdout == "" and get_unrecon_status_post.stderr == "")
            retries: 80
            delay: 25
            ignore_errors: yes

          - name: fail if previous task failed
            fail:
              msg:
                - "Unexpected failure while waiting for reconciled resources"
                - "{{get_unrecon_status_post.stderr}}"
            register: unexpected_failure
            when: (get_unrecon_status_post.stderr != "")

          # Get pod name to retrieve the logs after unlock
          - name: Get DM pod name
            shell: >-
              kubectl -n platform-deployment-manager get pods |
              awk '$1 ~ /^platform-deployment-manager/ {print $1}'
            environment:
              KUBECONFIG: "/etc/kubernetes/admin.conf"
            register: get_dm_pod_name_after

          # Get errors from pod logs again, but after unlock
          - name: Retrieve DM logs after unlock
            shell: >-
              kubectl -n platform-deployment-manager logs "{{ get_dm_pod_name_after.stdout }}" |
              awk '(/ERROR/ && !/validation/ && !/waiting/ && !/Reconciler error/&& !/unhandled/ && !(seen[$10, $NF]++)) {print}'
            environment:
              KUBECONFIG: "/etc/kubernetes/admin.conf"
            register: get_logs_after_unlock

          - name: Show errors from logs after unlock
            debug:
              msg:
              - "Pod log: {{get_logs_after_unlock.stdout}}"
              - "err: {{get_logs_after_unlock.stderr}}"

          when: ("Unlocking" in get_show_task_status.stdout)

        # If the task "Wait until unlock task triggered" failed,  we export some
        # useful information and fail the playbook with msg.
        - name: fail if unlock task not triggered
          fail:
            msg:
              - "Fail waiting for host unlock to be triggered. It could be due to DM config errors"
              - "UNRECONCILED resources: {{get_recon_status.stdout}}"
              - "Err_code= unrec_pre_unlock"
              - "{{get_logs_pre_unlock.stdout}}"
          register: fail_dm_pre_unlock
          when: ("Unlocking" not in get_show_task_status.stdout)

        # If we have unreconciled resources after unlock and after
        # waiting some time, it will fail displaying possible errors
        # from logs.
        - name: Fail if there are failures after unlock
          fail:
            msg:
              - "Fail waiting resources to be reconciled after unlock. It could be due to DM config errors"
              - "UNRECONCILED resources: {{get_unrecon_status_post.stdout}}"
              - "Err_code= unrec_after_unlock"
              - "{{get_logs_after_unlock.stdout}}"
          register: fail_dm_after_unlock
          when: ("Unlocking" in get_show_task_status.stdout
                 and get_unrecon_status_post.stdout != "")

      when:
        - initial_deploy
        - deploy_config is defined
        - "'subcloud' in get_distributed_cloud_role.stdout"
        - "'unlocked' not in current_administrativestate.stdout"
        - "'unlocked' in administrative_state"
