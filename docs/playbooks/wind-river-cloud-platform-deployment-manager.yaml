---
# SPDX-License-Identifier: Apache-2.0
# Copyright(c) 2021-2024 Wind River Systems, Inc.
- name: Deployment Manager Playbook
  hosts: all
  gather_facts: false
  become: false

  tasks:
    - set_fact:
        manager_chart: "{{ deployment_manager_chart | default('wind-river-cloud-platform-deployment-manager.tgz') }}"

    - set_fact:
        user_uploaded_artifacts: "{{ user_uploaded_artifacts | default(true) }}"

    - set_fact:
        ansible_port: "{{ ansible_port | default(22) }}"
        wait_for_timeout: " {{ wait_for_timeout | default(400) }}"
        boot_wait_time: " {{ boot_wait_time | default(180) }}"

    - set_fact:
        helm_chart_overrides: "{{ deployment_manager_overrides }}"
      when: deployment_manager_overrides is defined

    - set_fact:
        deploy_config: "{{ deployment_config }}"
        wait_for_dm_unlock: " {{ wait_for_dm_unlock| default(5) }}"
      when: deployment_config is defined

    - set_fact:
        manager_app_chart: "{{ manager_app_chart | default('/usr/local/share/applications/helm/deployment-manager-*.tgz') }}"

    - set_fact:
        dm_monitor_playbook: "{{ dm_monitor_playbook | default('/usr/local/share/applications/playbooks/dm-monitor-book.yaml') }}"

    - block:
      # Information to be used at final DM checks.
        - name: Get host name
          shell: |
            source /etc/platform/openrc; hostname
          register: get_host_name

        # In order to determine if the host is a subcloud's node
        - name: Get distributed_cloud_role
          shell: >-
            source /etc/platform/openrc; system show |
            awk '$2 ~ /^distributed_cloud_role/ {print $4}'
          register: get_distributed_cloud_role

        - name: Get software version
          shell: >-
            source /etc/platform/openrc; system show |
            awk '$2 ~ /^software_version/ {print $4}'
          register: get_software_version

        - name: Show dc_role and hostname
          debug:
            msg:
            - "hostname: {{get_host_name.stdout}}"
            - "dc_role: {{ get_distributed_cloud_role.stdout }}"
            - "software_version: {{ get_software_version.stdout }}"

    - block:
      # Copy required files up to the target host if this playbook is being
      # executed remotely.

      - block:
        # Download a copy of the deployment manager helm chart if the location
        # supplied by the end user references a git repo.

        - name: Create A Temporary Download Directory
          tempfile:
            state: directory
          register: temp
          delegate_to: localhost

        - name: Download Deployment Manager Helm Chart From Repo
          shell: "git archive --remote={{ manager_chart }} | tar -x -C {{ temp.path }}"
          delegate_to: localhost

        - name: Reference Downloaded Helm Chart
          set_fact:
            manager_chart: "{{ lookup('fileglob', '{{ temp.path }}/docs/charts/wind-river-cloud-platform-deployment-manager-*.tgz', wantlist=true) | first }}"

        when: manager_chart | regex_search("^https|^git")

      - name: Upload Deployment Manager Helm Chart
        copy:
          src: "{{ manager_chart }}"
          dest: /home/{{ ansible_ssh_user }}/
          owner: "{{ ansible_ssh_user }}"
          group: root
          mode: 0644

      - set_fact:
          manager_chart: "/home/{{ ansible_ssh_user }}/{{ manager_chart | basename }}"

      - name: Upload Deployment Manager Helm Chart Overrides
        copy:
          src: "{{ helm_chart_overrides }}"
          dest: /home/{{ ansible_ssh_user }}/
          group: root
          mode: 0644
        when: helm_chart_overrides is defined

      - set_fact:
          helm_chart_overrides: "/home/{{ ansible_ssh_user }}/{{ helm_chart_overrides | basename }}"
        when: helm_chart_overrides is defined

      - block:

        ## user not uploaded the artifacts
        ## check in subcloud /usr/local/share/applications/

        - set_fact:
            helm_chart_overrides: "/usr/local/share/applications/overrides/wind-river-cloud-platform-deployment-manager-overrides-subcloud.yaml"
            manager_chart: "/usr/local/share/applications/helm/wind-river-cloud-platform-deployment-manager-*.tgz"

        - name: Check if helm-chart file exists in subcloud
          shell: ls {{ manager_chart }}
          ignore_errors: yes
          register: helm_chart_file

        - name: Check if helm-overrides file exists in subcloud
          stat:
            path: "{{ helm_chart_overrides }}"
          register: helm_chart_overrides_file

        - name: Fail helm-chart and overrides not exists in subcloud
          fail:
            msg: "Helm-chart and overrides not exists in subcloud"
          when: (helm_chart_file.stdout | length == 0 or helm_chart_overrides_file.stat.exists == False)

        when: ("subcloud" in get_distributed_cloud_role.stdout and user_uploaded_artifacts is false)

      - name: Clean download directory
        file:
          path: "{{ temp.path }}"
          state: absent
        delegate_to: localhost
        when: temp.path is defined

      when: inventory_hostname != 'localhost'

    - name: Retrieve software version number
      shell: source /etc/build.info; echo $SW_VERSION
      register: sw_version_result

    - name: Fail if software version is not defined
      fail:
        msg: "SW_VERSION is missing in /etc/build.info"
      when: sw_version_result.stdout_lines|length == 0

    - name: Set software version and platform path
      set_fact:
        software_version: "{{ sw_version_result.stdout_lines[0] }}"
        platform_path: /opt/platform

    - name: Set config path facts
      set_fact:
        config_permdir: "{{ platform_path + '/config/' + software_version }}"

    - name: Check if host initial unlock is done
      stat:
        path: /etc/platform/.unlock_ready
      register: is_unlock_ready

    - name: Check if host is provisioned
      shell: |
        source /etc/platform/openrc;
        system host-show "{{ get_host_name.stdout}}" | awk '$2 ~ /^invprovision/ {print $4}'
      register: get_host_invprovision

    - name: Fail to update the deployment manager when host is upgrading
      fail:
        msg: "Fail to update the deployment manager when the host is upgrading"
      when: get_host_invprovision.stdout == 'upgrading'

    - name: Set initial configuration fact
      set_fact:
        initial_config_done: "{{ true if (get_host_invprovision.stdout == 'provisioned')
                        else false }}"

    - name: Mark the bootstrap as finalized
      file:
        path: "{{ config_permdir }}/.bootstrap_finalized"
        state: touch
      become: yes
      when: not initial_config_done

    - block:
      - name: Search for the pod of the Deployment Manager
        shell: |
          kubectl -n platform-deployment-manager get pods | grep platform-deployment-manager- | awk 'NR == 1 { print $1 }'
        environment:
          KUBECONFIG: "/etc/kubernetes/admin.conf"
        register: deployment_manager_pod_name

      - debug:
          msg: "{{ deployment_manager_pod_name.stdout }}"
      when: initial_config_done

    - block:
      # Install Deployment-Manager as System Application
      - name: Check deployment-manager app status
        shell: source /etc/platform/openrc; system application-list --nowrap | grep deployment-manager | awk '{ print $10 }'
        register: dm_app_status

      - name: Fail if the deployment manager application staus is not ready to apply
        fail:
          msg: >
            Deployment manager application is not in expected status:
            {{ dm_app_status.stdout }}
        when:
          - dm_app_status.stdout_lines |length > 0
          - dm_app_status.stdout != 'applied'
          - dm_app_status.stdout != 'uploaded'
          - dm_app_status.stdout != 'applying'

      - name: Upload deployment manager
        block:
        - name: Find deployment-manager app helm-chart in application directory
          find:
            paths: "{{ manager_app_chart | dirname }}"
            patterns: 'deployment-manager*.tgz'
          register: app_helm_chart

        - name: Fail deployment-manager app helm-chart does not exists
          fail:
            msg: "Deployment manger helm-chart does not exists in application directory"
          when: (app_helm_chart.files[0].path | length == 0)

        - name: Upload deployment-manager app Helm Chart
          shell: source /etc/platform/openrc; system application-upload {{ app_helm_chart.files[0].path }}

        - name: Wait deployment-manager app to become uploaded
          shell: source /etc/platform/openrc; system application-list --nowrap | grep deployment-manager | awk '{ print $10 }'
          register: app_upload_status
          retries: 10
          delay: 6
          until: ("uploaded" in app_upload_status.stdout)
          ignore_errors: yes

        - name: Fail if deployment-manager app upload fails
          fail:
            msg: "The upload of deployment-manager system app has failed.
                  Expected 'uploaded' state but the app is in
                  '{{ app_upload_status.stdout }}' state "
          when: not ("uploaded" in app_upload_status.stdout)

        when: dm_app_status.stdout_lines|length == 0

      - name: Apply deployment-manager app
        shell: source /etc/platform/openrc; system application-apply deployment-manager
        when: dm_app_status.stdout != 'applied' and dm_app_status.stdout != 'applying'

      - name: Wait deployment-manager app to become applied
        shell: source /etc/platform/openrc; system application-list --nowrap | grep deployment-manager | awk '{ print $10 }'
        register: app_apply_status
        retries: 30
        delay: 10
        until: ("applied" in app_apply_status.stdout)
        ignore_errors: yes

      - name: Fail if deployment-manager app apply fails
        fail:
          msg: "The apply of DM system app has failed.
                Expected 'applied' state but the app is in
                '{{ app_apply_status.stdout }}' state."
        when: not ("applied" in app_apply_status.stdout)

      when: get_software_version.stdout >= "24.09"

    - block:
      # Remove webhook and webhook service
      - name: Search webhook configurations for v1
        shell: |
          kubectl get ValidatingWebhookConfigurations validating-webhook-configuration --no-headers | awk 'NR == 1 { print $1 }'
        environment:
          KUBECONFIG: "/etc/kubernetes/admin.conf"
        ignore_errors: yes
        register: validation_webhook

      - name: Query for validating webhook configuration
        shell: |
          grep -iq 'validating-webhook-configuration' <<< {{ validation_webhook.stdout_lines }}
        ignore_errors: yes
        register: webhook_config_exists

      - name: Delete validating webhook configuration
        shell: |
          kubectl delete ValidatingWebhookConfigurations validating-webhook-configuration
        environment:
          KUBECONFIG: "/etc/kubernetes/admin.conf"
        when: webhook_config_exists.rc == 0

      - name: Search webhook service for v1
        shell: |
          kubectl get svc webhook-server-service -n platform-deployment-manager --no-headers | awk 'NR == 1 { print $1 }'
        environment:
          KUBECONFIG: "/etc/kubernetes/admin.conf"
        register: webhook_service

      - name: Query for webhook-server-service
        shell: |
          grep -iq 'webhook-server-service' <<< {{ webhook_service.stdout_lines }}
        ignore_errors: yes
        register: webhook_service_exists

      - name: Delete webhook-server-service
        shell: |
          kubectl delete svc webhook-server-service -n platform-deployment-manager
        environment:
          KUBECONFIG: "/etc/kubernetes/admin.conf"
        when: webhook_service_exists.rc == 0

      # Install Helm Based Deployment Manager
      - name: Install Helm Based Deployment Manager
        shell: KUBECONFIG=/etc/kubernetes/admin.conf /usr/sbin/helm upgrade --install deployment-manager {% if helm_chart_overrides is defined %}--values {{ helm_chart_overrides }}{% endif %} {{ manager_chart }}

      # Install or Upgrade DM-Monitor
      - name: "Install or Upgrade Helm Based DM-monitor"
        shell: "ansible-playbook {{ dm_monitor_playbook }}"
        become: yes
        ignore_errors: yes
        when: dm_monitor_playbook

      when:
        - get_software_version.stdout < "24.09"
        - "'subcloud' in get_distributed_cloud_role.stdout"
        - user_uploaded_artifacts is false

    # Delete old Deployment Manager pod if it was reinstalled,
    # the pod to delete may already be terminated by helm
    - name: Restart Deployment Manager if reinstalled
      command: >-
        kubectl -n platform-deployment-manager delete pods {{ deployment_manager_pod_name.stdout }}
      environment:
        KUBECONFIG: "/etc/kubernetes/admin.conf"
      when: deployment_manager_pod_name.stdout is defined and deployment_manager_pod_name.stdout
      failed_when: false

    - name: Wait for Deployment Manager to be ready
      shell: KUBECONFIG=/etc/kubernetes/admin.conf /bin/kubectl wait --namespace=platform-deployment-manager --for=condition=Ready pods --selector control-plane=controller-manager --timeout=60s
      register: wait_for_deployment_manager
      ignore_errors: true

    - block:
      - name: Search for the Deployment Manager pod name in case of failure
        shell: |
          kubectl -n platform-deployment-manager get pods | awk 'NR == 2 { print $1 }'
        environment:
          KUBECONFIG: "/etc/kubernetes/admin.conf"
        register: deployment_manager_pod_name_after_fail

      - name: Describe DM pod in case of download failed
        shell: |
          kubectl -n platform-deployment-manager describe pod "{{ deployment_manager_pod_name_after_fail.stdout }}" |
          awk '/Failed/ || /error/  {print $0}'
        environment:
          KUBECONFIG: "/etc/kubernetes/admin.conf"
        register: describe_dm_failed_pod

      - name: Fail if Deployment Manager pod is not ready
        fail:
          msg:
            - "Deployment manager pod are not ready"
            - "Check helm overrides files and ensure you can pull the image from the specified URL."
            - "Err_code= dm_pod_failed"
            - "Pod information: {{describe_dm_failed_pod.stdout}}"
      when: wait_for_deployment_manager.stderr != ""

    - block:
        - name: Upload Deployment Configuration File
          copy:
            src: "{{ deploy_config }}"
            dest: /home/{{ ansible_ssh_user }}/deployment-config.yaml
            owner: "{{ ansible_ssh_user }}"
            group: root
            mode: 0755
          when: inventory_hostname != 'localhost'

        - set_fact:
            deploy_config: "/home/{{ ansible_ssh_user }}/deployment-config.yaml"
          when: inventory_hostname != 'localhost'

        - name: Check for the presence of the subcloud enrollment completed
          stat:
            path: /var/run/.subcloud_enrollment_completed
          register: subcloud_enrollment_status

        - set_fact:
            subcloud_enrollment: "{{ subcloud_enrollment_status.stat.exists }}"

        - name: Configure required region value if not systemcontroller
          block:
          - name: Get region name from environment
            shell: source /etc/platform/openrc; echo $OS_REGION_NAME
            register: os_region_name_from_env

          - name: Show region name
            debug:
              msg:
              - "System region name : {{os_region_name_from_env.stdout}}"

          - name: Configure the region name in the deploy config
            command: |
              sed -i 's/^\(\s*OS_REGION_NAME:\).*/\1 {{ os_region_name_from_env.stdout }}/' {{ deploy_config }}
          when: >
            (get_distributed_cloud_role.stdout is not defined) or
            get_distributed_cloud_role.stdout != "systemcontroller"

        - wait_for:
            # Pause for an arbitrary amount of time to allow the deployment
            # manager to come up and download its certificates.  It needs to
            # restart during this process so the webhooks may not be ready when
            # we apply the config in the next steps.
            timeout: 10
            msg: Waiting for the Deployment Manager validation webhooks to start

        # Search for both cases:
        #   deploymentScope: "principal"
        # and
        #   deploymentScope:
        #    "principal"
        - name: Search for 'principal' scope in deploy_config
          command: grep -Pzo '\n\s*deploymentScope:\s*\n*(?:\s*#.*\n)*\s*(?:"principal"|"Principal"|"PRINCIPAL")\s*' {{ deploy_config }}
          changed_when: false
          failed_when: false
          register: scope_principal

        - name: Pre-check and process for subcloud enrollment
          block:
          - name: Fail if system subcloud enrollment but have principal in scope
            fail:
              msg: "Principal scope cannot used in subcloud enrollment."
            when: scope_principal.stdout_lines | length > 0

          - name: Retrieve all system resource names
            command: kubectl -n deployment get systems -o jsonpath='{.items[*].metadata.name}'
            environment:
              KUBECONFIG: "/etc/kubernetes/admin.conf"
            register: system_resource_names

          - name: Parse the system resource names into a list
            set_fact:
              system_resources: "{{ system_resource_names.stdout.split() }}"
            when: system_resource_names.stdout | length > 0

          - name: Debug the list of system resource names
            debug:
              msg: "System resources to be deleted: {{ system_resources }}"
            when: system_resources is defined

          - name: Delete all system resources
            command: |
              kubectl -n deployment delete system {{ item }}
            loop: "{{ system_resources }}"
            environment:
              KUBECONFIG: "/etc/kubernetes/admin.conf"
            when: system_resources is defined and system_resources | length > 0
          when: subcloud_enrollment

        # Check if there is any strategy-related alarm
        - name: Check if a VIM strategy is in progress
          shell: |
            source /etc/platform/openrc; fm alarm-list | grep 900.
          failed_when: false
          register: strategy_alarms
          when: scope_principal.stdout_lines | length > 0

        - name: Fail if there is a strategy in progress during a Day-2 operation
          fail:
            msg:
              - "VIM Strategy in progress."
              - "Wait until all the strategies are cleared before applying the system configuration update."
          when:
            - scope_principal.stdout_lines | length > 0
            - strategy_alarms.stdout != ""

        - name: Append config map for subcloud enrollment
          blockinfile:
            path: "{{ deploy_config }}"
            block: |

              ---
              apiVersion: v1
              kind: ConfigMap
              metadata:
                name: factory-install
                namespace: deployment
              data:
                factory-installed: "true"
                factory-config-finalized: "false"
            marker: ""
            insertafter: EOF
          when: subcloud_enrollment

        - name: Apply Deployment Configuration File
          shell: KUBECONFIG=/etc/kubernetes/admin.conf /bin/kubectl apply -f {{ deploy_config }}
          register: apply_deploy_config
          retries: 5
          delay: 10
          until: apply_deploy_config.rc == 0
          ignore_errors: true

        - name: Fail if config file failed to apply
          fail:
            msg:
              - "Failed to apply DM config file"
              - "Check syntax into config file. See documentation examples. "
              - "Err_code= dm_apply_failed"
              - "Config file error: {{apply_deploy_config.stderr}}"
          when: apply_deploy_config.rc != 0

        # Waiting task after apply config to avoid failures getting info
        - wait_for:
            timeout: 5
            msg: Waiting after apply DM config

        - name: Restart the deployment manager if subcloud enrollment
          command: >-
            kubectl -n platform-deployment-manager delete pods --selector control-plane=controller-manager
          environment:
            KUBECONFIG: "/etc/kubernetes/admin.conf"
          when: subcloud_enrollment

        # Check the current administrativestate
        - name: Get current administrativeState
          shell: |
            source /etc/platform/openrc;
            system host-show "{{ get_host_name.stdout}}" | awk '$2 ~ /^administrative/ {print $4}'
          register: current_administrativestate

      when: deploy_config is defined

    # Create default registry key in platform-deployment-manager for future image pulls
    - name: Get platform-deployment-manager namespace default registry key
      command: >-
        kubectl --kubeconfig=/etc/kubernetes/admin.conf get secret default-registry-key --namespace=platform-deployment-manager
      failed_when: false
      register: get_dm_default_registry_key

    - name: Copy default-registry-key to platform-deployment-manager namespace
      shell: >-
        kubectl get secret default-registry-key --namespace=kube-system -o yaml
        | sed 's/namespace: kube-system/namespace: platform-deployment-manager/'
        | kubectl apply --namespace=platform-deployment-manager -f -
      environment:
        KUBECONFIG: "/etc/kubernetes/admin.conf"
      when: get_dm_default_registry_key.stdout == ""

    # Pre-verification block: it will search for administrative state applied
    # in order to avoid waiting for unlock when config administrativeState=locked.
    # Search first in host resource config. If not present, search into hostprofile.
    - block:
        - name: Search administrativeState into host resource
          shell: |
            source /etc/platform/openrc;
            kubectl get host "{{ get_host_name.stdout}}" -n deployment -o=jsonpath='{.spec.administrativeState}'
          environment:
            KUBECONFIG: "/etc/kubernetes/admin.conf"
          register: get_host_adminstate
          ignore_errors: yes

        # If config not found in host -previous task- then search into hostprofile
        - name: Get hostprofile name for this host
          shell: |
            source /etc/platform/openrc;
            kubectl get host "{{get_host_name.stdout}}" -n deployment -o=jsonpath='{.spec.profile}'
          environment:
            KUBECONFIG: "/etc/kubernetes/admin.conf"
          register: get_hostprofile_name
          when: get_host_adminstate.stdout == ""
          ignore_errors: yes

        - name: Search administrativeState into hostprofile resource
          shell: |
            source /etc/platform/openrc;
            kubectl get hostprofile "{{get_hostprofile_name.stdout}}" -n deployment -o=jsonpath='{.spec.administrativeState}'
          environment:
            KUBECONFIG: "/etc/kubernetes/admin.conf"
          register: get_hostprofile_adminstate
          when: (get_hostprofile_name.stdout != "" and get_hostprofile_name.stderr == "")

        - set_fact:
            administrative_state: "{{get_host_adminstate.stdout + get_hostprofile_adminstate.stdout}}"
          when: (get_host_name.stderr == "" and get_hostprofile_name.stderr == "")

        - name: Show config administrativeState
          debug:
            msg:
            - "administrative state: {{administrative_state}}"

      when:
        - not initial_config_done
        - deploy_config is defined
        - "'subcloud' in get_distributed_cloud_role.stdout"
        - "'unlocked' not in current_administrativestate.stdout"


      # Main verification block.
      # Behavior:
      # This block waits until the host unlock is triggered.
      # - If the unlock is not triggered after a certain time, it might indicate DM failures.
      #   In this case, it proceeds to the 'get failure information' block to collect details
      #   about the potential cause.
      # - If the unlock process is in progress, it proceeds to the 'unlock verifications' block.
      #   Here, it waits for the host's port to be closed and then reopened. Following that, it checks
      #   the status of resources before completing.
    - block:

        # - If unlock task is triggered, is highly probable that the config applied is correct.
        # - The task fails by achieving max retries without calling the unlock.
        - name: Wait until unlock task triggered
          shell: |
            source /etc/platform/openrc;
            system host-show "{{ get_host_name.stdout}}" | awk '$2 ~ /^task/ {print $4}'
          register: get_show_task_status
          until: ("Unlocking" in get_show_task_status.stdout)
          retries: 60
          delay: "{{wait_for_dm_unlock}}"
          ignore_errors: yes

        - name: Show waiting unlock
          debug:
            msg:
            - "waiting: {{get_show_task_status.stdout}}"
            - "waiting: {{get_show_task_status.stderr}}"

        # Get failure information Block:
        # if unlock was not triggered, get information and make the playbook fail below
        - block:

          # Get a list of unreconciled resources pre unlock
          # This task is getting the last column as RECONCILED value
          # TODO(ecandotti): Modify this block to not retrieve the last column, but instead recognize the column number for RECONCILED in each resource.
          # TODO(ecandotti): This will eliminate the need to always have the RECONCILED column as the last column when a new state is added.
          - name: Retrieve kubectl resources in unreconciled status
            shell: >-
              kubectl -n deployment get datanetworks,hostprofiles,hosts,platformnetworks,systems,ptpinstances,ptpinterfaces |
              awk '$NF ~ /^false/ {print}'
            environment:
              KUBECONFIG: "/etc/kubernetes/admin.conf"
            register: get_recon_status

          - name: Show unreconciled resources
            debug:
              msg:
              - "recon: {{get_recon_status.stdout}}"

          # Get pod name to retrieve the logs
          - name: Get DM pod name
            shell: >-
              kubectl -n platform-deployment-manager get pods |
              awk '$1 ~ /^platform-deployment-manager/ {print $1}'
            environment:
              KUBECONFIG: "/etc/kubernetes/admin.conf"
            register: get_dm_pod_name

          # Get errors from pod logs. Searching for error lines into logs which:
          # - Contain 'ERROR' key word.
          # - Are not the same "Verb + value". Usually we see same error
          # - Are not validation or waiting errors (which could be temporal).
          #   multiple times in logs.
          - name: Retrieve DM logs
            shell: >-
              kubectl -n platform-deployment-manager logs "{{ get_dm_pod_name.stdout }}" |
              awk '(/ERROR/ && !/validation/ && !/waiting/ && !/Reconciler error/&& !(seen[$10, $NF]++)) {print}'
            environment:
              KUBECONFIG: "/etc/kubernetes/admin.conf"
            register: get_logs_pre_unlock

          - name: Show logs
            debug:
              msg:
              - "Pod log: {{get_logs_pre_unlock.stdout}}"
              - "err: {{get_logs_pre_unlock.stderr}}"

          when: ("Unlocking" not in get_show_task_status.stdout)

        # Unlock verifications Block:
        # if unlock was triggered, wait until unlock complete
        # and check resources and logs.
        - block:

          # check close-open port before starting the last checkings
          - block:
            - name: Waiting for SSH port to be closed due to unlock task
              local_action:
                module: wait_for
                  port={{ ansible_port }}
                  host={{ ansible_host }}
                  timeout={{ wait_for_timeout }}
                  delay=5
                  state=stopped

            - name: Waiting for SSH port to become open
              local_action:
                module: wait_for
                  port={{ ansible_port }}
                  host={{ ansible_host }}
                  timeout={{ wait_for_timeout }}
                  delay={{ boot_wait_time }}
                  state=started
              retries: 20
              delay: 20

            # Rescue block due to port timing issues:
            # It will check the administrative state and status of resources.
            # If the state is not "unlocked," log the information from the subcloud.
            # If the state is "unlocked," continue with the normal flow of the main block.
            rescue:
              - name: Waiting for SSH port to become open
                local_action:
                  module: wait_for
                    port={{ ansible_port }}
                    host={{ ansible_host }}
                    delay={{ boot_wait_time }}
                    timeout={{ wait_for_timeout }}
                    state=started
                retries: 20
                delay: 20

              - name: Wait for 300 seconds before checking status
                wait_for:
                  timeout: 300
                register: waiting_after_failures
                ignore_unreachable: true

              - name: Get post unlock state
                shell: >-
                  kubectl -n deployment get hosts "{{ get_host_name.stdout}}" |
                  awk 'NR == 1 { for (i=1; i<=NF; i++) { if ($i == "ADMINISTRATIVE") { col = i; break } } } NR > 1 { print $col }'
                environment:
                  KUBECONFIG: "/etc/kubernetes/admin.conf"
                register: get_final_admin_state
                until: ( administrative_state == get_final_admin_state.stdout)
                retries: 4
                delay: 15
                ignore_errors: yes

              - name: Retrieve kubectl resources reconciled status
                shell: >-
                  (kubectl -n deployment get datanetworks,platformnetworks,systems,ptpinstances,ptpinterfaces;
                  kubectl -n deployment get hosts "{{ get_host_name.stdout}}")
                environment:
                  KUBECONFIG: "/etc/kubernetes/admin.conf"
                register: resources_status
                ignore_errors: yes

              - name: Showing status after unlock
                debug:
                  msg:
                  - "WARNING. Final status after reboot: {{get_final_admin_state.stdout}}"
                  - "It's possible that the status after attempting to unlock is not as desired:"
                  - "{{resources_status.stdout}}"
                when: ( administrative_state != get_final_admin_state.stdout)

          # Waiting task after unlock to catch right status
          # This simple task usually doesn't fail. However, in some
          # cases, after unlocking, we may encounter two reboots. In such
          # situations, this task might fail due to a loss of connection.
          # If that happens, the following block will wait for the
          # subsequent reboot.
          - name: Wait for {{ boot_wait_time }} seconds to ensure not affecting host status
            wait_for:
              timeout: "{{ boot_wait_time }}"
            register: waiting_after_reboot
            # Unreachable: Ignore a task failure due to the host instance being ’UNREACHABLE’ 
            # with the ignore_unreachable keyword. Ansible ignores the task errors but continues
            # to execute future tasks against the unreachable host. This usually happens 
            # if the host is network-inaccessible. For example, during a second reboot that
            # temporarily disrupts SSH connectivity, Ansible will mark it as unreachable since 
            # it can’t connect to the host at all.
            ignore_unreachable: true
            # Failed: It can ignore errors when the task is failed, except those syntax errors, 
            # undefined variable errors, connection failures, execution issues.
            # This occurs if Ansible can reach the host, but the task itself encounters an issue. This 
            # could happen if the host is reachable but not fully ready to respond, or if a required
            # port isn’t open yet.
            ignore_errors: true
            
          # Retry block: sometimes system reboots twice
          # It will take some extra time.
          - block:
            - name: Waiting for SSH port to become open on a second reboot
              local_action:
                module: wait_for
                  port={{ ansible_port }}
                  host={{ ansible_host }}
                  delay={{ boot_wait_time }}
                  timeout={{ wait_for_timeout }}
                  state=started
              retries: 20
              delay: 20

            # Waiting task after unlock to catch the right status
            - name: Waiting after second reboot for {{ boot_wait_time }} seconds to ensure not affecting host status
              wait_for:
                timeout: "{{ boot_wait_time }}"
              register: waiting_after_new_reboot
              ignore_errors: true

            - name: Fail in case of new loss of connection
              fail:
                msg:
                  - "Unexpected loss of connection. Check subcloud status manually."
                  - "Then, use subcloud reconfig command to complete deployment"
              register: new_reboot
              when: waiting_after_new_reboot.failed
 
            when: waiting_after_reboot.failed
 
          - name: Set retries to check resource reconciled status for simplex
            set_fact:
              host_reconcile_retries: 80

          - name: Check number of hosts from deployment namespace
            shell: kubectl get hosts -n deployment |  grep -v NAME  | wc -l
            environment:
              KUBECONFIG: "/etc/kubernetes/admin.conf"
            register: hosts_count
            until: hosts_count.stdout != 0
            retries: 20
            delay: 30
            failed_when: false

          - name: Fail to get hosts from cluster
            fail:
              msg: "Failed to get host resource from cluster."
            when: hosts_count.stdout == 0

          - name: Wait for subcloud hosts available if not simplex
            block:
            - name: Set system type for monitor the deloyment
              set_fact:
                host_offline_retries: >
                  {{ 60 if hosts_count.stdout | int == 2 else 120 }}
                host_reconcile_retries: >
                  {{ 120 if  hosts_count.stdout | int == 2 else 240 }}

            - name: Get offline hosts if not simplex
              shell: >
                kubectl get hosts -o custom-columns='NAME:.metadata.name,
                OPERATIONAL:.status.availabilityStatus' -n deployment |
                awk '$NF ~ /offline/ {print}'
              environment:
                KUBECONFIG: "/etc/kubernetes/admin.conf"
              register: get_offline_hosts
              until: >
                (get_offline_hosts.stdout == "" and
                get_offline_hosts.stderr == "")
              retries: "{{ host_offline_retries }}"
              delay: 60
              failed_when: false

            - name: Fail if any host remain offline
              fail:
                msg: >
                  Host remain offline: {{ get_offline_hosts.stdout }}
              when: get_offline_hosts.stdout != ""
            when: hosts_count.stdout != "1"

          # After reboot, wait for resource reconciliation. This ensures the
          # playbook doesn't fail if resources are properly reconciled.
          - name: Define resource list based on version
            set_fact:
              resources: "{{ 'systems,datanetworks,platformnetworks,ptpinstances,ptpinterfaces' if get_software_version.stdout < '24.09'
                        else 'systems,datanetworks,platformnetworks,ptpinstances,ptpinterfaces,addresspools' }}"

          - name: Get not reconciled resource
            shell: >
              (kubectl get "{{ resources }}"
              -o custom-columns='NAME:.metadata.name,RECONCILED:.status.reconciled'
              -n deployment;
              kubectl -n deployment get hosts "{{ get_host_name.stdout }}" -o
              custom-columns='NAME:.metadata.name,RECONCILED:.status.reconciled') |
              awk '$NF ~ /^false/ {print}'
            environment:
              KUBECONFIG: "/etc/kubernetes/admin.conf"
            register: get_unrecon_status_post
            until: >
              (get_unrecon_status_post.stdout == "" and
              get_unrecon_status_post.stderr == "")
            retries: "{{ host_reconcile_retries }}"
            delay: 30
            failed_when: false

          - name: Fail if fail to get resource reconciled status
            fail:
              msg:
                - "Unexpected failure while waiting for reconciled resources"
                - "{{get_unrecon_status_post.stderr}}"
            when: (get_unrecon_status_post.stderr != "")

          # Get pod name to retrieve the logs after unlock
          - name: Get DM pod name
            shell: >-
              kubectl -n platform-deployment-manager get pods |
              awk '$1 ~ /^platform-deployment-manager/ {print $1}'
            environment:
              KUBECONFIG: "/etc/kubernetes/admin.conf"
            register: get_dm_pod_name_after

          # Get errors from pod logs again, but after unlock
          - name: Retrieve DM logs after unlock
            shell: >-
              kubectl -n platform-deployment-manager logs "{{ get_dm_pod_name_after.stdout }}" |
              awk '(/ERROR/ && !/validation/ && !/waiting/ && !/Reconciler error/&& !/unhandled/ && !(seen[$10, $NF]++)) {print}'
            environment:
              KUBECONFIG: "/etc/kubernetes/admin.conf"
            register: get_logs_after_unlock

          - name: Show errors from logs after unlock
            debug:
              msg:
              - "Pod log: {{get_logs_after_unlock.stdout}}"
              - "err: {{get_logs_after_unlock.stderr}}"

          when: ("Unlocking" in get_show_task_status.stdout)

        # If the task "Wait until unlock task triggered" failed,  we export some
        # useful information and fail the playbook with msg.
        - name: fail if unlock task not triggered
          fail:
            msg:
              - "Fail waiting for host unlock to be triggered. It could be due to DM config errors"
              - "UNRECONCILED resources: {{get_recon_status.stdout}}"
              - "Err_code= unrec_pre_unlock"
              - "{{get_logs_pre_unlock.stdout}}"
          register: fail_dm_pre_unlock
          when: ("Unlocking" not in get_show_task_status.stdout)

        # If we have unreconciled resources after unlock and after
        # waiting some time, it will fail displaying possible errors
        # from logs.
        - name: Fail if there are failures after unlock
          fail:
            msg:
              - "Fail waiting resources to be reconciled after unlock. It could be due to DM config errors"
              - "UNRECONCILED resources: {{get_unrecon_status_post.stdout}}"
              - "Err_code= unrec_after_unlock"
              - "{{get_logs_after_unlock.stdout}}"
          register: fail_dm_after_unlock
          when: ("Unlocking" in get_show_task_status.stdout
                 and get_unrecon_status_post.stdout != "")

      when:
        - not initial_config_done
        - deploy_config is defined
        - "'subcloud' in get_distributed_cloud_role.stdout"
        - "'unlocked' not in current_administrativestate.stdout"
        - "'unlocked' in administrative_state"
