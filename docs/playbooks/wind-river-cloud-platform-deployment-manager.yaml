# SPDX-License-Identifier: Apache-2.0
# Copyright(c) 2021-2022 Wind River Systems, Inc.
- name: Deployment Manager Playbook
  hosts: all
  gather_facts: false
  become: false
  tasks:
    - set_fact:
        manager_chart: "{{ deployment_manager_chart | default('wind-river-cloud-platform-deployment-manager.tgz') }}"

    - set_fact:
        ansible_port: "{{ ansible_port | default(22) }}"
        wait_for_timeout: " {{ wait_for_timeout | default(900) }}"
        boot_wait_time: " {{ boot_wait_time | default(200) }}"

    - set_fact:
        helm_chart_overrides: "{{ deployment_manager_overrides }}"
      when: deployment_manager_overrides is defined
    
    - set_fact:
        deploy_config: "{{ deployment_config }}"
        wait_for_dm_unlock: " {{ wait_for_dm_unlock| default(5) }}"
      when: deployment_config is defined

    - block:
      # Information to be used at final DM checks.
        - name: Get host name
          shell: |
            source /etc/platform/openrc; hostname
          register: get_host_name 

        # In order to determine if the host is a subcloud's node
        - name: Get distributed_cloud_role
          shell: >-
            source /etc/platform/openrc; system show |
            awk '$2 ~ /^distributed_cloud_role/ {print $4}'
          register: get_distributed_cloud_role

        - name: Show dc_role and hostname
          debug:
            msg:
            - "hostname: {{get_host_name.stdout}}"
            - "dc_role: {{ get_distributed_cloud_role.stdout }}"

    - block:
      # Copy required files up to the target host if this playbook is being
      # executed remotely.

      - block:
        # Download a copy of the deployment manager helm chart if the location
        # supplied by the end user references a git repo.

        - name: Create A Temporary Download Directory
          tempfile:
            state: directory
          register: temp
          delegate_to: localhost

        - name: Download Deployment Manager Helm Chart From Repo
          shell: "git archive --remote={{ manager_chart }} | tar -x -C {{ temp.path }}"
          delegate_to: localhost

        - name: Reference Downloaded Helm Chart
          set_fact:
            manager_chart: "{{ lookup('fileglob', '{{ temp.path }}/docs/charts/wind-river-cloud-platform-deployment-manager-*.tgz', wantlist=true) | first }}"

        when: manager_chart | regex_search("^https|^git")

      - name: Upload Deployment Manager Helm Chart
        copy:
          src: "{{ manager_chart }}"
          dest: /home/{{ ansible_ssh_user }}/
          owner: "{{ ansible_ssh_user }}"
          group: root
          mode: 0644

      - set_fact:
          manager_chart: "/home/{{ ansible_ssh_user }}/{{ manager_chart | basename }}"

      - name: Upload Deployment Manager Helm Chart Overrides
        copy:
          src: "{{ helm_chart_overrides }}"
          dest: /home/{{ ansible_ssh_user }}/
          group: root
          mode: 0644
        when: helm_chart_overrides is defined

      - set_fact:
          helm_chart_overrides: "/home/{{ ansible_ssh_user }}/{{ helm_chart_overrides | basename }}"
        when: helm_chart_overrides is defined
      
      - name: Clean download directory
        file:
          path: "{{ temp.path }}"
          state: absent
        delegate_to: localhost
        when: temp.path is defined

      when: inventory_hostname != 'localhost'

    - name: Retrieve software version number
      shell: source /etc/build.info; echo $SW_VERSION
      register: sw_version_result

    - name: Fail if software version is not defined
      fail:
        msg: "SW_VERSION is missing in /etc/build.info"
      when: sw_version_result.stdout_lines|length == 0

    - name: Set software version and platform path
      set_fact:
        software_version: "{{ sw_version_result.stdout_lines[0] }}"
        platform_path: /opt/platform

    - name: Set config path facts
      set_fact:
        config_permdir: "{{ platform_path + '/config/' + software_version }}"

    - name: Mark the bootstrap as finalized
      file:
        path: "{{ config_permdir }}/.bootstrap_finalized"
        state: touch
      become: yes

    # Check if DM already exists in either helmv2 or helmv3
    - name: Get list of releases in helmv2
      command: >-
        /usr/local/sbin/helmv2-cli -- helm list --output json
      environment:
        KUBECONFIG: "/etc/kubernetes/admin.conf"
      ignore_errors: yes
      register: helmv2_releases

    - name: Query for DM release in helmv2
      shell: |
        grep -iq '"name":"deployment-manager"' <<< {{ helmv2_releases.stdout_lines }}
      ignore_errors: yes
      register: helmv2_dm_exists

    - name: Get list of releases in helmv3
      command: >-
        /usr/sbin/helm list --output json
      environment:
        KUBECONFIG: "/etc/kubernetes/admin.conf"
      ignore_errors: yes
      register: helmv3_releases

    - name: Query for DM release in helmv3
      shell: |
        grep -iq '"name":"deployment-manager"' <<< {{ helmv3_releases.stdout_lines }}
      ignore_errors: yes
      register: helmv3_dm_exists

    # Follow a different reinstallation procedure if DM is installed in helmv2
    - block:
      - name: Get armada pod name
        command: >-
          kubectl get pods -n armada -o custom-columns=NAME:metadata.name --no-headers
        environment:
          KUBECONFIG: "/etc/kubernetes/admin.conf"
        register: armada_pod

      - name: Show armada pod
        debug:
          msg:
          - "armada pod ID: {{ armada_pod.stdout }}"

      - name: Copy files into tiller container if using helmv2
        command: "{{ item }}"
        with_items:
          - kubectl -n armada cp
            {{ manager_chart }}
            {{ armada_pod.stdout }}:/tmp/.
            -c tiller
          - kubectl -n armada cp
            {% if helm_chart_overrides is defined %}{{ helm_chart_overrides }}{% endif %}
            {{ armada_pod.stdout }}:/tmp/.
            -c tiller
        environment:
          KUBECONFIG: "/etc/kubernetes/admin.conf"

      - name: Reinstall Deployment Manager (helmv2)
        command: >-
          /usr/local/sbin/helmv2-cli -- helm upgrade
          --install deployment-manager
          --values /tmp/{{ helm_chart_overrides | basename }}
          /tmp/{{ manager_chart | basename }}
        environment:
          KUBECONFIG: "/etc/kubernetes/admin.conf"

      when: helmv2_dm_exists.rc == 0

    # Remove v1 webhook and webhook service
    - block:
      - name: Search webhook configurations for v1
        shell: |
          kubectl get ValidatingWebhookConfigurations validating-webhook-configuration --no-headers | awk 'NR == 1 { print $1 }'
        environment:
          KUBECONFIG: "/etc/kubernetes/admin.conf"
        ignore_errors: yes
        register: validation_webhook

      - name: Query for validating webhook configuration
        shell: |
          grep -iq 'validating-webhook-configuration' <<< {{ validation_webhook.stdout_lines }}
        ignore_errors: yes
        register: webhook_config_exists

      - name: Delete validating webhook configuration
        shell: |
          kubectl delete ValidatingWebhookConfigurations validating-webhook-configuration
        environment:
          KUBECONFIG: "/etc/kubernetes/admin.conf"
        when: webhook_config_exists.rc == 0

      - name: Search webhook service for v1
        shell: |
          kubectl get svc webhook-server-service -n platform-deployment-manager --no-headers | awk 'NR == 1 { print $1 }'
        environment:
          KUBECONFIG: "/etc/kubernetes/admin.conf"
        register: webhook_service

      - name: Query for webhook-server-service
        shell: |
          grep -iq 'webhook-server-service' <<< {{ webhook_service.stdout_lines }}
        ignore_errors: yes
        register: webhook_service_exists

      - name: Delete webhook-server-service
        shell: |
          kubectl delete svc webhook-server-service -n platform-deployment-manager
        environment:
          KUBECONFIG: "/etc/kubernetes/admin.conf"
        when: webhook_service_exists.rc == 0

      when: helmv2_dm_exists.rc != 0

    - name: Install Deployment Manager
      shell: KUBECONFIG=/etc/kubernetes/admin.conf /usr/sbin/helm upgrade --install deployment-manager {% if helm_chart_overrides is defined %}--values {{ helm_chart_overrides }}{% endif %} {{ manager_chart }}
      when: helmv2_dm_exists.rc != 0

    # Restart Deployment Manager if it was reinstalled
    - block:
      - name: Search for the pod of the Deployment Manager
        shell: |
          kubectl -n platform-deployment-manager get pods | grep platform-deployment-manager- | awk 'NR == 1 { print $1 }'
        environment:
          KUBECONFIG: "/etc/kubernetes/admin.conf"
        register: deployment_manager_pod_name

      - debug:
          msg: "{{ deployment_manager_pod_name.stdout }}"

      - name: Restart Deployment Manager if reinstalled
        command: >-
          kubectl -n platform-deployment-manager delete pods {{ deployment_manager_pod_name.stdout }}
        environment:
          KUBECONFIG: "/etc/kubernetes/admin.conf"
        when: deployment_manager_pod_name.stdout

      when: helmv2_dm_exists.rc == 0 or helmv3_dm_exists.rc == 0

    - name: Wait for Deployment Manager to be ready
      shell: KUBECONFIG=/etc/kubernetes/admin.conf /bin/kubectl wait --namespace=platform-deployment-manager --for=condition=Ready pods --selector control-plane=controller-manager --timeout=60s
      register: wait_for_deployment_manager
      ignore_errors: true

    - block:
      - name: Search for the Deployment Manager pod name in case of failure
        shell: |
          kubectl -n platform-deployment-manager get pods | awk 'NR == 2 { print $1 }'
        environment:
          KUBECONFIG: "/etc/kubernetes/admin.conf"
        register: deployment_manager_pod_name_after_fail

      - name: Describe DM pod in case of download failed
        shell: |
          kubectl -n platform-deployment-manager describe pod "{{ deployment_manager_pod_name_after_fail.stdout }}" |
          awk '/Failed/ || /error/  {print $0}'
        environment:
          KUBECONFIG: "/etc/kubernetes/admin.conf"
        register: describe_dm_failed_pod

      - name: Fail if Deployment Manager pod is not ready
        fail:
          msg:
            - "Deployment manager pod are not ready"
            - "Check helm overrides files and ensure you can pull the image from the specified URL."
            - "Err_code= dm_pod_failed"
            - "Pod information: {{describe_dm_failed_pod.stdout}}"
      when: wait_for_deployment_manager.stderr != ""

    - block:
        - name: Upload Deployment Configuration File
          copy:
            src: "{{ deploy_config }}"
            dest: /home/{{ ansible_ssh_user }}/deployment-config.yaml
            owner: "{{ ansible_ssh_user }}"
            group: root
            mode: 0755
          when: inventory_hostname != 'localhost'

        - set_fact:
            deploy_config: "/home/{{ ansible_ssh_user }}/deployment-config.yaml"
          when: inventory_hostname != 'localhost'

        - wait_for:
            # Pause for an arbitrary amount of time to allow the deployment
            # manager to come up and download its certificates.  It needs to
            # restart during this process so the webhooks may not be ready when
            # we apply the config in the next steps.
            timeout: 10
            msg: Waiting for the Deployment Manager validation webhooks to start

        - name: Apply Deployment Configuration File
          shell: KUBECONFIG=/etc/kubernetes/admin.conf /bin/kubectl apply -f {{ deploy_config }}
          register: apply_deploy_config
          retries: 5
          delay: 10
          until: apply_deploy_config.rc == 0
          ignore_errors: true

        - name: Fail if config file failed to apply
          fail:
            msg:
              - "Failed to apply DM config file"
              - "Check syntax into config file. See documentation examples. "
              - "Err_code= dm_apply_failed"
              - "Config file error: {{apply_deploy_config.stderr}}"
          when: apply_deploy_config.rc != 0

        # Waiting task after apply config to avoid failures getting info
        - wait_for:
            timeout: 5
            msg: Waiting after apply DM config

        # Check the current administrativestate
        - name: Get current administrativeState
          shell: |
            source /etc/platform/openrc;
            system host-show "{{ get_host_name.stdout}}" | awk '$2 ~ /^administrative/ {print $4}'
          register: current_administrativestate

      when: deploy_config is defined

    # Create default registry key in platform-deployment-manager for future image pulls
    - name: Get platform-deployment-manager namespace default registry key
      command: >-
        kubectl --kubeconfig=/etc/kubernetes/admin.conf get secret default-registry-key --namespace=platform-deployment-manager
      failed_when: false
      register: get_dm_default_registry_key

    - name: Copy default-registry-key to platform-deployment-manager namespace
      shell: >-
        kubectl get secret default-registry-key --namespace=kube-system -o yaml
        | sed 's/namespace: kube-system/namespace: platform-deployment-manager/'
        | kubectl apply --namespace=platform-deployment-manager -f -
      environment:
        KUBECONFIG: "/etc/kubernetes/admin.conf"
      when: get_dm_default_registry_key.stdout == ""

    # Pre-verification block: it will search for administrative state applied
    # in order to avoid waiting for unlock when config administrativeState=locked.
    # Search first in host resource config. If not present, search into hostprofile.
    - block:
        - name: Search administrativeState into host resource
          shell: |
            source /etc/platform/openrc;
            kubectl get host "{{ get_host_name.stdout}}" -n deployment -o=jsonpath='{.spec.administrativeState}'
          environment:
            KUBECONFIG: "/etc/kubernetes/admin.conf"
          register: get_host_adminstate
          ignore_errors: yes

        # If config not found in host -previous task- then search into hostprofile
        - name: Get hostprofile name for this host
          shell: |
            source /etc/platform/openrc;
            kubectl get host "{{get_host_name.stdout}}" -n deployment -o=jsonpath='{.spec.profile}'
          environment:
            KUBECONFIG: "/etc/kubernetes/admin.conf"
          register: get_hostprofile_name
          when: get_host_adminstate.stdout == ""
          ignore_errors: yes

        - name: Search administrativeState into hostprofile resource
          shell: |
            source /etc/platform/openrc;
            kubectl get hostprofile "{{get_hostprofile_name.stdout}}" -n deployment -o=jsonpath='{.spec.administrativeState}'
          environment:
            KUBECONFIG: "/etc/kubernetes/admin.conf"
          register: get_hostprofile_adminstate
          when: (get_hostprofile_name.stdout != "" and get_hostprofile_name.stderr == "")

        - set_fact:
            administrative_state: "{{get_host_adminstate.stdout + get_hostprofile_adminstate.stdout}}"
          when: (get_host_name.stderr == "" and get_hostprofile_name.stderr == "")

        - name: Show config administrativeState
          debug:
            msg:
            - "administrative state: {{administrative_state}}"

      when: (deploy_config is defined and "subcloud" in get_distributed_cloud_role.stdout
             and "unlocked" not in current_administrativestate.stdout)

      # Verification block
    - block:
        # - If unlock task is triggered, is highly probable that the config applied is correct.
        # - If the task fails (by achieving max retries without calling the unlock), the
        #   playbook will fail but it will collect some information before exiting.
        - name: Wait until unlock task triggered
          shell: |
            source /etc/platform/openrc;
            system host-show "{{ get_host_name.stdout}}" | awk '$2 ~ /^task/ {print $4}'
          register: get_show_task_status
          until: ("Unlocking" in get_show_task_status.stdout)
          retries: 60
          delay: "{{wait_for_dm_unlock}}"
          ignore_errors: yes

        - name: Show waiting unlock
          debug:
            msg:
            - "waiting: {{get_show_task_status.stdout}}"
            - "waiting: {{get_show_task_status.stderr}}"

        # if unlock was not triggered, get information and make the playbook fail below
        - block:
          # Get a list of unreconciled resources pre unlock
          # This task is getting the last column as RECONCILED value
          - name: Retrieve kubectl resources in unreconciled status
            shell: >-
              kubectl -n deployment get datanetworks,hostprofiles,hosts,platformnetworks,systems,ptpinstances,ptpinterfaces |
              awk '$NF ~ /^false/ {print}'
            environment:
              KUBECONFIG: "/etc/kubernetes/admin.conf"
            register: get_recon_status

          - name: Show unreconciled resources
            debug:
              msg:
              - "recon: {{get_recon_status.stdout}}"

          # Get pod name to retrieve the logs
          - name: Get DM pod name
            shell: >-
              kubectl -n platform-deployment-manager get pods |
              awk '$1 ~ /^platform-deployment-manager/ {print $1}'
            environment:
              KUBECONFIG: "/etc/kubernetes/admin.conf"
            register: get_dm_pod_name

          # Get errors from pod logs. Searching for error lines into logs which:
          # - Contain 'ERROR' key word.
          # - Are not the same "Verb + value". Usually we see same error
          # - Are not validation or waiting errors (which could be temporal).
          #   multiple times in logs.
          - name: Retrieve DM logs
            shell: >-
              kubectl -n platform-deployment-manager logs "{{ get_dm_pod_name.stdout }}" |
              awk '(/ERROR/ && !/validation/ && !/waiting/ && !/Reconciler error/&& !(seen[$10, $NF]++)) {print}'
            environment:
              KUBECONFIG: "/etc/kubernetes/admin.conf"
            register: get_logs_pre_unlock

          - name: Show logs
            debug:
              msg:
              - "Pod log: {{get_logs_pre_unlock.stdout}}"
              - "err: {{get_logs_pre_unlock.stderr}}"

          when: ("Unlocking" not in get_show_task_status.stdout)

        # if unlock was triggered, wait until unlock complete
        # and check resources and logs.
        - block:
          # check close-open port before starting last verifications
          - name: Waiting for port to be closed due to unlock task
            local_action:
              module: wait_for
                port={{ ansible_port }}
                host={{ ansible_host }}
                delay={{ boot_wait_time }}
                timeout={{ wait_for_timeout }}
                state=stopped
            retries: 20
            delay: 20

          - name: Waiting for port to become open
            local_action:
              module: wait_for
                port={{ ansible_port }}
                host={{ ansible_host }}
                delay={{ boot_wait_time }}
                timeout={{ wait_for_timeout }}
                state=started
            retries: 40
            delay: 20

          # Waiting task after unlock to catch right status
          # This simple task usually doesn't fail. However, in some
          # cases, after unlocking, we may encounter two reboots. In such
          # situations, this task might fail due to a loss of connection.
          # If that happens, the following block will wait for the
          # subsequent reboot.
          - wait_for:
              timeout: 450
              msg: Waiting after unlock due to apply DM config
            register: waiting_after_reboot
            ignore_errors: true

          # Retry block: sometimes system reboots twice
          - block:
            - name: Waiting for port to become open on second reboot
              local_action:
                module: wait_for
                  port={{ ansible_port }}
                  host={{ ansible_host }}
                  delay={{ boot_wait_time }}
                  timeout={{ wait_for_timeout }}
                  state=started
              retries: 40
              delay: 20

            # Waiting task after unlock to catch right status
            - wait_for:
                timeout: 450
                msg: Waiting after unlock due to apply DM config
              register: waiting_after_new_reboot
              ignore_errors: true

            - name: Fail in case of new loss of connection
              fail:
                msg:
                  - "Unexpected loss of connection. Check subcloud status manually."
                  - "Then, use subcloud reconfig command to complete deployment"
              register: new_reboot
              when: waiting_after_new_reboot.failed

            when: waiting_after_reboot.failed


          # After reboot, wait some time for resources to be reconciled.
          # If we have all of them reconciled, playbook won't fail.
          # This task is getting the last column as RECONCILED status value
          - name: Retrieve kubectl resources reconciled status
            shell: >-
              (kubectl -n deployment get datanetworks,platformnetworks,systems,ptpinstances,ptpinterfaces;
               kubectl -n deployment get hosts "{{ get_host_name.stdout}}") |
               awk '$NF ~ /^false/ {print}'
            environment:
              KUBECONFIG: "/etc/kubernetes/admin.conf"
            register: get_unrecon_status_post
            until: (get_unrecon_status_post.stdout == "" and get_unrecon_status_post.stderr == "")
            retries: 80
            delay: 25
            ignore_errors: yes

          - name: fail if previous task failed
            fail:
              msg:
                - "Unexpected failure while waiting for reconciled resources"
                - "{{get_unrecon_status_post.stderr}}"
            register: unexpected_failure
            when: (get_unrecon_status_post.stderr != "")

          # Get pod name to retrieve the logs after unlock
          - name: Get DM pod name
            shell: >-
              kubectl -n platform-deployment-manager get pods |
              awk '$1 ~ /^platform-deployment-manager/ {print $1}'
            environment:
              KUBECONFIG: "/etc/kubernetes/admin.conf"
            register: get_dm_pod_name_after

          # Get errors from pod logs again, but after unlock
          - name: Retrieve DM logs after unlock
            shell: >-
              kubectl -n platform-deployment-manager logs "{{ get_dm_pod_name_after.stdout }}" |
              awk '(/ERROR/ && !/validation/ && !/waiting/ && !/Reconciler error/&& !/unhandled/ && !(seen[$10, $NF]++)) {print}'
            environment:
              KUBECONFIG: "/etc/kubernetes/admin.conf"
            register: get_logs_after_unlock

          - name: Show errors from logs after unlock
            debug:
              msg:
              - "Pod log: {{get_logs_after_unlock.stdout}}"
              - "err: {{get_logs_after_unlock.stderr}}"

          when: ("Unlocking" in get_show_task_status.stdout)

        # If the task "Wait until unlock task triggered" failed,  we export some
        # useful information and fail the playbook with msg.
        - name: fail if unlock task not triggered
          fail:
            msg:
              - "Fail waiting for host unlock to be triggered. It could be due to DM config errors"
              - "UNRECONCILED resources: {{get_recon_status.stdout}}"
              - "Err_code= unrec_pre_unlock"
              - "{{get_logs_pre_unlock.stdout}}"
          register: fail_dm_pre_unlock
          when: ("Unlocking" not in get_show_task_status.stdout)

        # If we have unreconciled resources after unlock and after
        # waiting some time, it will fail displaying possible errors
        # from logs.
        - name: Fail if there are failures after unlock
          fail:
            msg:
              - "Fail waiting resources to be reconciled after unlock. It could be due to DM config errors"
              - "UNRECONCILED resources: {{get_unrecon_status_post.stdout}}"
              - "Err_code= unrec_after_unlock"
              - "{{get_logs_after_unlock.stdout}}"
          register: fail_dm_after_unlock
          when: ("Unlocking" in get_show_task_status.stdout
                 and get_unrecon_status_post.stdout != "")

      when: (deploy_config is defined and "subcloud" in get_distributed_cloud_role.stdout
             and "unlocked" not in current_administrativestate.stdout
             and "unlocked" in administrative_state)
